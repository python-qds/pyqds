{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"qdscreen \u00b6 Remove redundancy in your categorical variables and increase your models performance. qdscreen provides a python implementation of the Quasi-determinism screening algorithm (also known as qds-BNSL ) from T.Rahier's PhD thesis, 2018. Most data scientists are familiar with the concept of correlation between continuous variables. This concept extends to categorical variables, and is known as functional dependency in the field of relational databases mining. We also name it determinism in the context of Machine Learning and Statistics, to indicate that when a random variable X is known then the value of another variable Y is determined with absolute certainty. \"Quasi-\" determinism is an extension of this concept to handle noise or extremely rare cases in data. qdscreen is able to detect and remove (quasi-)deterministic relationships in a dataset: either as a preprocessing step in any general-purpose data science pipeline or as an accelerator of a Bayesian Network Structure Learning method such as pyGOBN Installing \u00b6 > pip install qdscreen Usage \u00b6 1. Remove correlated variables \u00b6 a. Strict determinism \u00b6 Let's consider the following dataset: import pandas as pd df = pd . DataFrame ({ 'U' : [ \"a\" , \"b\" , \"d\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"d\" , \"c\" ], 'V' : [ \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"c\" ], 'W' : [ \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"b\" ], 'X' : [ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"c\" , \"c\" , \"a\" , \"b\" , \"c\" ], 'Y' : [ \"b\" , \"b\" , \"c\" , \"c\" , \"b\" , \"a\" , \"a\" , \"b\" , \"c\" , \"a\" ], 'Z' : [ \"a\" , \"a\" , \"b\" , \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"a\" , \"b\" ] }) We can detect correlated categorical variables (functional dependencies): from qdscreen import qd_screen # detect strict deterministic relationships qd_forest = qd_screen ( df ) print ( qd_forest ) yields QDForest (6 vars): - 3 roots (1+2*): U*, X*, Z - 3 other nodes: V, W, Y U \u2514\u2500 V \u2514\u2500 W X \u2514\u2500 Y So with only features U , and X we should be able to predict V , W , and Y . Z is a root but has no children so it does not help. We can create a feature selection model from this deterministic forest object: print ( \"Columns in df: %s \" % list ( df . columns )) # fit a feature selection model feat_selector = qd_forest . fit_selector_model ( df ) # use it to filter... only_important_features = feat_selector . remove_qd ( df ) print ( \"Columns in only_important_features: %s \" % list ( only_important_features . columns )) # or to restore/predict restored_full_df = feat_selector . predict_qd ( only_important_features ) print ( \"Columns in restored_full_df: %s \" % list ( restored_full_df . columns )) # note that the order of columns differs from origin pd . testing . assert_frame_equal ( df , restored_full_df [ df . columns ]) yields Columns in df: ['U', 'V', 'W', 'X', 'Y', 'Z'] Columns in only_important_features: ['U', 'X', 'Z'] Columns in restored_full_df: ['U', 'X', 'Z', 'V', 'W', 'Y'] b. Quasi determinism \u00b6 In the above example, we used the default settings of qd_screen . By default only deterministic relationships are detected, which means that only variables that can perfectly be predicted (without loss of information) from others in the dataset are removed. In real-world datasets, some noise can occur in the data, or some very rare cases might happen, that you may wish to discard. Let's first look at the strength of the various relationships thanks to keep_stats=True : # the same than above, but this time remember the various indicators qd_forest = qd_screen ( df , keep_stats = True ) # display them print ( qd_forest . stats ) yields: Statistics computed for dataset: U V W X Y Z 0 a a a a b a 1 b b b a b a ...(10 rows) Entropies (H): U 1.970951 V 1.570951 W 0.881291 X 1.570951 Y 1.570951 Z 0.970951 dtype: float64 Conditional entropies (Hcond = H(row|col)): U V W X Y Z U 0.000000 0.400000 1.089660 0.875489 0.875489 1.475489 V 0.000000 0.000000 0.689660 0.875489 0.875489 1.200000 W 0.000000 0.000000 0.000000 0.875489 0.875489 0.875489 X 0.475489 0.875489 1.565148 0.000000 0.000000 0.875489 Y 0.475489 0.875489 1.565148 0.000000 0.000000 0.875489 Z 0.475489 0.600000 0.965148 0.275489 0.275489 0.000000 Relative conditional entropies (Hcond_rel = H(row|col)/H(row)): U V W X Y Z U 0.000000 0.202948 0.552860 0.444196 0.444196 0.748618 V 0.000000 0.000000 0.439008 0.557299 0.557299 0.763869 W 0.000000 0.000000 0.000000 0.993416 0.993416 0.993416 X 0.302676 0.557299 0.996307 0.000000 0.000000 0.557299 Y 0.302676 0.557299 0.996307 0.000000 0.000000 0.557299 Z 0.489715 0.617951 0.994024 0.283731 0.283731 0.000000 With the last table for example we see that variable Z 's entropies decreases drastically to reach 28% of its initial entropy, if X or Y is known. So if we use quasi-determinism with relative threshold of 29% Z would be eliminated. Another, easier way to see this is to use the plotting functions: qd_forest . stats . plot () c. Integrating with scikit-learn \u00b6 scikit-learn is one of the most popular machine learning frameworks in python. It comes with a concept of Pipeline allowing you to chain several operators to make a model. qdscreen provides a QDSSelector class for easy integration. It works exactly like other feature selection models in scikit-learn (e.g. ) 2. Learn a Bayesian Network structure \u00b6 TODO see #6 . Main features / benefits \u00b6 A feature selection algorithm able to eliminate quasi-deterministic relationships a base version compliant with numpy and pandas datasets a scikit-learn compliant version (numpy only) An accelerator for Bayesian Network Structure Learning tasks See Also \u00b6 Bayesian Network libraries in python: pyGOBN (MIT license) pgmpy (MIT license) pomegranate (MIT license) bayespy (MIT license) Functional dependencies libraries in python: fd_miner , an algorithm that was used in this paper . The repository contains a list of reference datasets too. FDTool a python 2 algorithm to mine for functional dependencies, equivalences and candidate keys. From this paper . functional-dependencies functional-dependency-finder connects to a MySQL db and finds functional dependencies. Other libs for probabilistic inference: pyjags (GPLv2 license) edward (Apache License, Version 2.0) Stackoverflow discussions: detecting normal forms canonical cover Others \u00b6 Do you like this library ? You might also like smarie's other python libraries Want to contribute ? \u00b6 Details on the github page: https://github.com/python-qds/qdscreen","title":"Home"},{"location":"#qdscreen","text":"Remove redundancy in your categorical variables and increase your models performance. qdscreen provides a python implementation of the Quasi-determinism screening algorithm (also known as qds-BNSL ) from T.Rahier's PhD thesis, 2018. Most data scientists are familiar with the concept of correlation between continuous variables. This concept extends to categorical variables, and is known as functional dependency in the field of relational databases mining. We also name it determinism in the context of Machine Learning and Statistics, to indicate that when a random variable X is known then the value of another variable Y is determined with absolute certainty. \"Quasi-\" determinism is an extension of this concept to handle noise or extremely rare cases in data. qdscreen is able to detect and remove (quasi-)deterministic relationships in a dataset: either as a preprocessing step in any general-purpose data science pipeline or as an accelerator of a Bayesian Network Structure Learning method such as pyGOBN","title":"qdscreen"},{"location":"#installing","text":"> pip install qdscreen","title":"Installing"},{"location":"#usage","text":"","title":"Usage"},{"location":"#1-remove-correlated-variables","text":"","title":"1. Remove correlated variables"},{"location":"#a-strict-determinism","text":"Let's consider the following dataset: import pandas as pd df = pd . DataFrame ({ 'U' : [ \"a\" , \"b\" , \"d\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"d\" , \"c\" ], 'V' : [ \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"c\" ], 'W' : [ \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"b\" ], 'X' : [ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"c\" , \"c\" , \"a\" , \"b\" , \"c\" ], 'Y' : [ \"b\" , \"b\" , \"c\" , \"c\" , \"b\" , \"a\" , \"a\" , \"b\" , \"c\" , \"a\" ], 'Z' : [ \"a\" , \"a\" , \"b\" , \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"a\" , \"b\" ] }) We can detect correlated categorical variables (functional dependencies): from qdscreen import qd_screen # detect strict deterministic relationships qd_forest = qd_screen ( df ) print ( qd_forest ) yields QDForest (6 vars): - 3 roots (1+2*): U*, X*, Z - 3 other nodes: V, W, Y U \u2514\u2500 V \u2514\u2500 W X \u2514\u2500 Y So with only features U , and X we should be able to predict V , W , and Y . Z is a root but has no children so it does not help. We can create a feature selection model from this deterministic forest object: print ( \"Columns in df: %s \" % list ( df . columns )) # fit a feature selection model feat_selector = qd_forest . fit_selector_model ( df ) # use it to filter... only_important_features = feat_selector . remove_qd ( df ) print ( \"Columns in only_important_features: %s \" % list ( only_important_features . columns )) # or to restore/predict restored_full_df = feat_selector . predict_qd ( only_important_features ) print ( \"Columns in restored_full_df: %s \" % list ( restored_full_df . columns )) # note that the order of columns differs from origin pd . testing . assert_frame_equal ( df , restored_full_df [ df . columns ]) yields Columns in df: ['U', 'V', 'W', 'X', 'Y', 'Z'] Columns in only_important_features: ['U', 'X', 'Z'] Columns in restored_full_df: ['U', 'X', 'Z', 'V', 'W', 'Y']","title":"a. Strict determinism"},{"location":"#b-quasi-determinism","text":"In the above example, we used the default settings of qd_screen . By default only deterministic relationships are detected, which means that only variables that can perfectly be predicted (without loss of information) from others in the dataset are removed. In real-world datasets, some noise can occur in the data, or some very rare cases might happen, that you may wish to discard. Let's first look at the strength of the various relationships thanks to keep_stats=True : # the same than above, but this time remember the various indicators qd_forest = qd_screen ( df , keep_stats = True ) # display them print ( qd_forest . stats ) yields: Statistics computed for dataset: U V W X Y Z 0 a a a a b a 1 b b b a b a ...(10 rows) Entropies (H): U 1.970951 V 1.570951 W 0.881291 X 1.570951 Y 1.570951 Z 0.970951 dtype: float64 Conditional entropies (Hcond = H(row|col)): U V W X Y Z U 0.000000 0.400000 1.089660 0.875489 0.875489 1.475489 V 0.000000 0.000000 0.689660 0.875489 0.875489 1.200000 W 0.000000 0.000000 0.000000 0.875489 0.875489 0.875489 X 0.475489 0.875489 1.565148 0.000000 0.000000 0.875489 Y 0.475489 0.875489 1.565148 0.000000 0.000000 0.875489 Z 0.475489 0.600000 0.965148 0.275489 0.275489 0.000000 Relative conditional entropies (Hcond_rel = H(row|col)/H(row)): U V W X Y Z U 0.000000 0.202948 0.552860 0.444196 0.444196 0.748618 V 0.000000 0.000000 0.439008 0.557299 0.557299 0.763869 W 0.000000 0.000000 0.000000 0.993416 0.993416 0.993416 X 0.302676 0.557299 0.996307 0.000000 0.000000 0.557299 Y 0.302676 0.557299 0.996307 0.000000 0.000000 0.557299 Z 0.489715 0.617951 0.994024 0.283731 0.283731 0.000000 With the last table for example we see that variable Z 's entropies decreases drastically to reach 28% of its initial entropy, if X or Y is known. So if we use quasi-determinism with relative threshold of 29% Z would be eliminated. Another, easier way to see this is to use the plotting functions: qd_forest . stats . plot ()","title":"b. Quasi determinism"},{"location":"#c-integrating-with-scikit-learn","text":"scikit-learn is one of the most popular machine learning frameworks in python. It comes with a concept of Pipeline allowing you to chain several operators to make a model. qdscreen provides a QDSSelector class for easy integration. It works exactly like other feature selection models in scikit-learn (e.g. )","title":"c. Integrating with scikit-learn"},{"location":"#2-learn-a-bayesian-network-structure","text":"TODO see #6 .","title":"2. Learn a Bayesian Network structure"},{"location":"#main-features-benefits","text":"A feature selection algorithm able to eliminate quasi-deterministic relationships a base version compliant with numpy and pandas datasets a scikit-learn compliant version (numpy only) An accelerator for Bayesian Network Structure Learning tasks","title":"Main features / benefits"},{"location":"#see-also","text":"Bayesian Network libraries in python: pyGOBN (MIT license) pgmpy (MIT license) pomegranate (MIT license) bayespy (MIT license) Functional dependencies libraries in python: fd_miner , an algorithm that was used in this paper . The repository contains a list of reference datasets too. FDTool a python 2 algorithm to mine for functional dependencies, equivalences and candidate keys. From this paper . functional-dependencies functional-dependency-finder connects to a MySQL db and finds functional dependencies. Other libs for probabilistic inference: pyjags (GPLv2 license) edward (Apache License, Version 2.0) Stackoverflow discussions: detecting normal forms canonical cover","title":"See Also"},{"location":"#others","text":"Do you like this library ? You might also like smarie's other python libraries","title":"Others"},{"location":"#want-to-contribute","text":"Details on the github page: https://github.com/python-qds/qdscreen","title":"Want to contribute ?"},{"location":"api_reference/","text":"API reference \u00b6 In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package. todo \u00b6 TODO Parameters: todo : TODO","title":"API reference"},{"location":"api_reference/#api-reference","text":"In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package.","title":"API reference"},{"location":"api_reference/#todo","text":"TODO Parameters: todo : TODO","title":"todo"},{"location":"changelog/","text":"Changelog \u00b6 0.1.0 (in progress) - First draft \u00b6 Initial release with: Scikit-learn compliant feature selector QDSSelector . Fixes #1 Method qdeterscreen to get the adjacency matrix of the qd-forest. Support for both pandas dataframes and numpy arrays as input. Fixes #2 A package level __version__ attribute. Fixes #3 Added py.typed for PEP561 compliance. Fixed #4 Initial setup.py and setup.cfg","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#010-in-progress-first-draft","text":"Initial release with: Scikit-learn compliant feature selector QDSSelector . Fixes #1 Method qdeterscreen to get the adjacency matrix of the qd-forest. Support for both pandas dataframes and numpy arrays as input. Fixes #2 A package level __version__ attribute. Fixes #3 Added py.typed for PEP561 compliance. Fixed #4 Initial setup.py and setup.cfg","title":"0.1.0 (in progress) - First draft"},{"location":"long_description/","text":"qdscreen \u00b6 Remove redundancy in your categorical variables and increase your models performance. qdscreen is a python implementation of the Quasi-determinism screening algorithm from T.Rahier's PhD thesis, 2018. The documentation for users is available here: https://python-qds.github.io/qdscreen/ A readme for developers is available here: https://github.com/python-qds/qdscreen","title":"qdscreen"},{"location":"long_description/#qdscreen","text":"Remove redundancy in your categorical variables and increase your models performance. qdscreen is a python implementation of the Quasi-determinism screening algorithm from T.Rahier's PhD thesis, 2018. The documentation for users is available here: https://python-qds.github.io/qdscreen/ A readme for developers is available here: https://github.com/python-qds/qdscreen","title":"qdscreen"}]}